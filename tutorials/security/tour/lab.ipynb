{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPIDS Academy Lab: GPU Security Analytics\n",
    "## Mapping 2.6GB of Zeek connection logs with cuDF and Graphistry\n",
    "\n",
    "The below exercises take you through using GPU RAPIDS ecosystem technologies to quickly load a large network security log dump into a notebook (cuDF), analyze the involved IPs, and visualize them (Graphistry).\n",
    "\n",
    "This notebook is part of the security analytics track's introductory session in RAPIDS Academy. RAPIDS Academy is a group of GPU computing leaders working together to provide tutorials and trainings for learning GPU analytics.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* **Video + chat**: See email/Slack: RAPIDS.ai Slack channel group + Zoom invite + GPU environment\n",
    "* **Schedule**: Introduction (Zoom), then 10min per task, with 5-10min discussion inbetween each task\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* **Python**: Comfortable with basic Python\n",
    "* **PyData**: Helpful but not necessary: Familiarity with Jupyter Notebooks & Pandas\n",
    "* **Security**: Minimal\n",
    "\n",
    "#### Topics\n",
    "\n",
    "* **Introduction: Jupyter Python notebooks**\n",
    "  * Writing, running, & saving cells, `nvidia-smi` & GPU dashboard\n",
    "* **Task 1: Starting & loading data**\n",
    "  * `help()`, `%%time`, `cudf.read_csv()`, memory management\n",
    "  * Advanced: comparing to `pandas.read_csv` and speedup via `usecols`\n",
    "* **Task 2: Inspection & analysis** \n",
    "  * `head()`, `[[]]`, `describe()`, `count()`, `sort()`, `len()`\n",
    "  * Advanced: `unique()`, `value_counts`\n",
    "* **Task 3: Shaping & visualization**\n",
    "  * `drop_duplicates`, `groupby().agg()`, `graphistry.plot()`\n",
    "  * Advanced: `graphistry.hypergraph`\n",
    "\n",
    "### After\n",
    "* Solution notebook\n",
    "* Slack channel\n",
    "* [Subscribe to future sessions](learnrapids.com): Multi-GPU, ...\n",
    "* Guide our roadmap! RAPIDS Academy + RAPIDS ecosystem survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import RAPIDS and prepare data\n",
    "\n",
    "* For Graphistry, get a free account at [alpha.graphistry.com](https://alpha.graphistry.com) and put in graphistry.register() call below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install graphistry client library if not available \n",
    "# NOTE: No local GPU needed as it uses a remote graphistry GPU server of your choice\n",
    "# ! pip install --user graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf, json, graphistry, pandas as pd\n",
    "from collections import OrderedDict\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "graphistry.register(\n",
    "    api=3,\n",
    "    username='*****', \n",
    "    password='*****')\n",
    "\n",
    "{'cudf': cudf.__version__, 'graphistry': graphistry.__version__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f conn.log conn.log.gz\n",
    "! echo \"Downloading data...\"\n",
    "! wget https://www.secrepo.com/maccdc2012/conn.log.gz\n",
    "! ls -alh conn.log.gz \n",
    "! echo \"Decompressing data...\"\n",
    "! gunzip conn.log.gz\n",
    "! ls -alh conn.log\n",
    "! echo \"DONE: DOWNLOADED DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"generating sample conn10.log ...\"\n",
    "! head -n 10 ./conn.log > conn10.log\n",
    "! echo \"generating sample conn1K.log ...\"\n",
    "! head -n 1000 ./conn.log > conn1K.log\n",
    "! echo \"generating sample conn1M.log ...\"\n",
    "! head -n 1000000 ./conn.log > conn1M.log\n",
    "! echo \"generating sample conn5M.logg ...\"\n",
    "! head -n 5000000 ./conn.log > conn5M.log\n",
    "! ls -alh conn*\n",
    "! echo \"DONE: SAMPLES GENERATED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick demo A: Jupyter\n",
    "\n",
    "1. Add cell: `+` button\n",
    "1. Run (ctrl-enter): \n",
    "```\n",
    "x = 1 + 2\n",
    "x\n",
    "```\n",
    "1. Run shell command: `ls -alh` and then `nvidia-smi`\n",
    "1. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1 + 1\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Demo B: GPU Dashboard\n",
    "\n",
    "In the left Jupyterlab menu, find the icon for the GPU dashboard extension and open the tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Load data\n",
    "\n",
    "* Read the 2.6GB file `conn.log` into a cuDF GPU dataframe using `cudf.read_csv`\n",
    "* Count the unique IPs\n",
    "* Compare the speed relative against CPU pandas\n",
    "* Intermediate + advanced: Keep GPU memory down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a: Load the data... without leaking GPU memory\n",
    "\n",
    "Load the data using `cudf.read_csv()`. It will be under the wrong format, initially.\n",
    "\n",
    "Your task is to do extra work around keeping GPU memory use under control. \n",
    "* At the end of every cell, set your dataframe variables to `None` (ex: `gdf = None`) so the garbage collector can free the memory\n",
    "* You can check GPU memory use via the dashboard and `! nvidia-smi`\n",
    "\n",
    "Start by loading sample `./conn1K.log`, and when ready, try `./conn1M.log`, `./conn5M.log`, and the full `./conn.log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Reference\n",
    "Print the last 3 lines of `./conn1K.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log')\n",
    "\n",
    "print(gdf.tail(3))\n",
    "\n",
    "gdf=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Task\n",
    "\n",
    "* Print the last 3 lines of `./conn5M.log` using `cudf.read_csv`\n",
    "* Use `! nvidia-smi` or the GPU monitor to check that memory consumption is low after\n",
    "\n",
    "After, try on `./conn.log` and see if it can hold the 2.6GB in memory. (RAPIDS has a max str size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn5M.log')\n",
    "\n",
    "print(gdf.tail(3))\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b: Load 500MB\n",
    "\n",
    "Load the data using tab separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "! ls -alh conn1K.log conn5M.log conn.log\n",
    "! echo \"lines  1K: `wc -l ./conn1K.log`\"\n",
    "! echo \"lines  5M: `wc -l ./conn5M.log`\"\n",
    "! echo \"lines all: `wc -l ./conn.log`\"\n",
    "! echo\n",
    "! echo \"===========================\"\n",
    "! echo \"./conn1K.log first 3 lines:\"\n",
    "! head -n 3 ./conn1K.log\n",
    "! echo\n",
    "! echo \"===========================\"\n",
    "! echo \"via cudf.read_csv()\"\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', sep='\\t')\n",
    "print(gdf.head(3))\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Task\n",
    "1. Read `help(cudf.read_csv)` to learn about the delimiter parameter\n",
    "1. Use the delimiter option to read the file with tab separation (`\"\\t\"`) \n",
    "1. Get it right with `./conn1K.log` and then try `./conn5M.log`\n",
    "1. ... remember to clear out your GPU memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cudf.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', sep='\\t')\n",
    "\n",
    "print(gdf.head(3))\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1c: Format & load 2.6GB\n",
    "\n",
    "`read_csv` has many useful parameters to create data that is cleaner, friendlier, smaller, and faster. \n",
    "\n",
    "By using them, we will be able to load the whole dataset with out crashing, fairly quickly, and get native operations on top!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Reference\n",
    "\n",
    "* Read about parameters `names`, `dtypes`, `colnames`, and byte ranges in `cudf.read_csv()`\n",
    "* Load the Zeek format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(cudf.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration',\n",
    "        'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "        'local_orig?',\n",
    "        'missed_bytes', 'history', 'orig_pkts',\n",
    "        'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "\n",
    "dtypes=OrderedDict([\n",
    "    ('ts', 'int64'), ('uid', 'str'),\n",
    "    ('id.orig_h', 'str'), ('id.orig_p', 'int32'), ('id.resp_h', 'str'), ('id.resp_p', 'int32'),\n",
    "    ('proto', 'str'), ('duration', 'float64'),\n",
    "    ('orig_bytes', 'int64'), ('resp_bytes', 'int64'),\n",
    "    ('conn_state', 'str'), ('local_orig?', 'str'), ('local_resp?', 'str'),\n",
    "    ('missed_bytes', 'int64'), ('history', 'str'),\n",
    "    ('orig_pkts', 'int64'), ('orig_ip_bytes', 'int64'), ('resp_pkts', 'int64'), ('resp_ip_bytes', 'int64'),\n",
    "    ('tunnel_parents', 'str')\n",
    "])\n",
    "\n",
    "# optional\n",
    "cols_subset = [\n",
    "    'ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', \n",
    "    'proto', 'duration', 'orig_bytes', 'resp_bytes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Task\n",
    "\n",
    "* Plug in `names` and `dtypes` to `read_csv`\n",
    "* ... Take care not to leak GPU memory, and try first on `./conn1K.log` before doing `./conn1M.log` / `./conn5M.log` / the full `./conn.log`.\n",
    "\n",
    "Advanced: Only load in the columns you will use for your analysis and compare the impact on memory & load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log',  sep='\\t',\n",
    "    names=cols,\n",
    "    dtypes=dtypes,\n",
    "    usecols=cols_subset,\n",
    "    na_values=[None, '-', '-','(empty)'])\n",
    "\n",
    "print(gdf.dtypes)\n",
    "\n",
    "gdf.head(3)\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Analytics & Wrangling\n",
    "\n",
    "CPU Pandas operators largely carry over to GPU cuDF. We'll take a look at activity by top IPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Column manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Reference\n",
    "* df with subset of cols: `gdf[['col1', 'col2']]`\n",
    "* get one col: `gdf['col1']`\n",
    "* get df/col length: `len(gdf)` / `len(gs)`\n",
    "* series of unique elements from a series: `gs.unique()`\n",
    "* stats on one series: `gs.min()`, `gs.max()`, `gs.sum()`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Task\n",
    "\n",
    "* For the column of IPs `id.resp_h`, how many unique IPs are there?\n",
    "* Intermediate/advanced: If you have time after 2b/c, for the column of bytes `orig_bytes`, what is the biggest payload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### id.resp_h unique value count\n",
    "\n",
    "gdf = cudf.read_csv('./conn.log', \n",
    "              sep='\\t', \n",
    "              names=cols,\n",
    "              dtypes=dtypes,\n",
    "              usecols=['id.resp_h'],\n",
    "              na_values=[None, '-', '-','(empty)'])\n",
    "\n",
    "unique_resp_ips = gdf['id.resp_h'].unique()\n",
    "\n",
    "print('# unique', len(unique_resp_ips))\n",
    "\n",
    "print(unique_resp_ips[:10])\n",
    "\n",
    "unique_resp_ips = None\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### col orig_bytes max\n",
    "\n",
    "gdf = cudf.read_csv('./conn.log', \n",
    "              sep='\\t', \n",
    "              names=cols,\n",
    "              dtypes=dtypes,\n",
    "              usecols=['orig_bytes'],\n",
    "              na_values=[None, '-', '-','(empty)'])\n",
    "\n",
    "mx = gdf['orig_bytes'].max()\n",
    "\n",
    "print('max orig_bytes', mx)\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b: Group by & column summaries\n",
    "\n",
    "Quite powerful, you can group dataframe rows and get summary statistics for each group.\n",
    "\n",
    "In this task, we'll summarize flows between different computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Reference\n",
    "\n",
    "Pattern\n",
    "\n",
    "```python\n",
    "gdf\\\n",
    "    .groupby(['col1', 'col2', ...])\\\n",
    "    .agg({\n",
    "        'col2': ['min', 'max'],\n",
    "        'col3': 'min',\n",
    "        'col4': ['count', 'mean', 'nunique'],\n",
    "    })\\\n",
    "    .reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Task\n",
    "\n",
    "Compute summaries for `./conn1K.log` then the full `./conn.log` for:\n",
    "\n",
    "* `ts`: # entries, min/max/mean time\n",
    "* `id.resp_p`: min/max/nunique\n",
    "* `duration`: min/max/mean\n",
    "* `orig_bytes`: min/max/mean/sum\n",
    "\n",
    "Advanced: Which are the biggest exfils? Longest? SSH?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn.log', \n",
    "              sep='\\t', \n",
    "              names=cols,\n",
    "              dtypes=dtypes,\n",
    "              usecols=cols_subset,\n",
    "              na_values=[None, '-', '-','(empty)'])\n",
    "\n",
    "out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "    .agg({\n",
    "        'ts': ['count', 'min', 'max', 'mean'],\n",
    "        'uid': 'nunique',\n",
    "        'id.resp_p': ['min', 'max', 'nunique'],\n",
    "        'proto': ['nunique'],\n",
    "        'duration': ['min', 'max', 'mean', 'sum'],\n",
    "        'orig_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "        'resp_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "gdf = None\n",
    "print(out.shape)\n",
    "print(out.dtypes)\n",
    "print(out.head(3))\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualize!\n",
    "\n",
    "Graphistry lets you plot many points, and even more interesting, relationships, and visually filter + cluster them on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Run Graphistry\n",
    "\n",
    "(Precoded)\n",
    "\n",
    "* Nodes: IPs\n",
    "* Edges: Summarized flows\n",
    "\n",
    "Try to -\n",
    "\n",
    "* Pan/zoom: Similar to Google Maps - click/drag/scroll (or pinch); recenter\n",
    "* Inspect: click on a node/edge, open table inspector\n",
    "* Advanced: histogram, filter, cluster, color\n",
    "* Advanced: rendering + clustering settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_groupby():\n",
    "\n",
    "    gdf = cudf.read_csv('./conn.log', \n",
    "                  sep='\\t', \n",
    "                  names=cols,\n",
    "                  dtypes=dtypes,\n",
    "                  usecols=cols_subset,\n",
    "                  na_values=[None, '-', '-','(empty)'])\n",
    "\n",
    "    out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "        .agg({\n",
    "            'ts': ['count', 'min', 'max', 'mean'],\n",
    "            'uid': 'nunique',\n",
    "            'id.resp_p': ['min', 'max', 'nunique'],\n",
    "            'proto': ['nunique'],\n",
    "            'duration': ['min', 'max', 'mean', 'sum'],\n",
    "            'orig_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "            'resp_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "        }).reset_index()\n",
    "\n",
    "\n",
    "    ########### Data cleaning: normal column names and times as actual timestamps\n",
    "\n",
    "    out.columns = out.columns.to_flat_index() # -> col_name = (col, stat)\n",
    "    out.columns = [ '%s_%s' % c for c in out.columns ]\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        'id.resp_h_': 'id.resp_h',\n",
    "        'id.orig_h_': 'id.orig_h',\n",
    "    })\n",
    "\n",
    "    out['ts_min'] = cudf.Series(pd.to_datetime((out['ts_min']*1000000000).to_pandas()))\n",
    "    out['ts_max'] = cudf.Series(pd.to_datetime((out['ts_max']*1000000000).to_pandas()))\n",
    "    out['ts_mean'] = cudf.Series(pd.to_datetime((out['ts_mean']*1000000000).to_pandas()))\n",
    "    \n",
    "    return out\n",
    "\n",
    "out = compute_groupby()\n",
    "    \n",
    "print('# rows', len(out))\n",
    "print('dtypes', out.dtypes)\n",
    "print(out.head(3))\n",
    "\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = compute_groupby()\n",
    "\n",
    "g = graphistry.edges(gdf).bind(source='id.orig_h', destination='id.resp_h')\n",
    "\n",
    "gdf = None\n",
    "\n",
    "g.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Map SSH activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Copy & rename `compute_groupby` as `compute_groupby2` \n",
    "* Filter connections to just SSH traffic (port 22) based on column `id.resp_p` \n",
    "* Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Reference\n",
    "\n",
    "Hint: To filter, `gdf2 = gdf[ gdf['some_col'] == some_val]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_groupby2():\n",
    "\n",
    "    gdf = cudf.read_csv('./conn.log', \n",
    "                  sep='\\t', \n",
    "                  names=cols,\n",
    "                  dtypes=dtypes,\n",
    "                  usecols=cols_subset,\n",
    "                  na_values=[None, '-', '-','(empty)'])\n",
    "    \n",
    "    gdf = gdf[ gdf['id.resp_p'] == 22 ]\n",
    "\n",
    "    out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "        .agg({\n",
    "            'ts': ['count', 'min', 'max', 'mean'],\n",
    "            'uid': 'nunique',\n",
    "            'id.resp_p': ['min', 'max', 'nunique'],\n",
    "            'proto': ['nunique'],\n",
    "            'duration': ['min', 'max', 'mean', 'sum'],\n",
    "            'orig_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "            'resp_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "        }).reset_index()\n",
    "\n",
    "\n",
    "    ########### Data cleaning: normal column names and times as actual timestamps\n",
    "\n",
    "    out.columns = out.columns.to_flat_index() # -> col_name = (col, stat)\n",
    "    out.columns = [ '%s_%s' % c for c in out.columns ]\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        'id.resp_h_': 'id.resp_h',\n",
    "        'id.orig_h_': 'id.orig_h',\n",
    "    })\n",
    "\n",
    "    out['ts_min'] = cudf.Series(pd.to_datetime((out['ts_min']*1000000000).to_pandas()))\n",
    "    out['ts_max'] = cudf.Series(pd.to_datetime((out['ts_max']*1000000000).to_pandas()))\n",
    "    out['ts_mean'] = cudf.Series(pd.to_datetime((out['ts_mean']*1000000000).to_pandas()))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = compute_groupby2()\n",
    "    \n",
    "print('# rows', len(out))\n",
    "print('dtypes', out.dtypes)\n",
    "print(out.head(3))\n",
    "\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = compute_groupby2()\n",
    "\n",
    "g = graphistry.edges(gdf).bind(source='id.orig_h', destination='id.resp_h')\n",
    "\n",
    "gdf = None\n",
    "\n",
    "g.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After\n",
    "\n",
    "#### Do\n",
    "* Solution notebook\n",
    "* Slack channel\n",
    "* [Subscribe to future sessions](learnrapids.com): Multi-GPU, ...\n",
    "* Guide our roadmap! RAPIDS Academy + RAPIDS ecosystem survey\n",
    "\n",
    "#### References\n",
    "* [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n",
    "* 2.6GB of Zeek connection logs (22M rows) from https://www.secrepo.com/ \n",
    "* RAPIDS docs: https://docs.rapids.ai/api/cudf/stable/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (RAPIDS)",
   "language": "python",
   "name": "rapids"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
