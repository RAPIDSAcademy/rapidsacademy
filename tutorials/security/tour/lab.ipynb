{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPIDS Academy Lab: GPU Security Analytics\n",
    "## Mapping 2.6GB of Zeek connection logs with cuDF and Graphistry\n",
    "\n",
    "The below exercises take you through using GPU RAPIDS ecosystem technologies to quickly load a large network security log dump into a notebook (cuDF), analyze the involved IPs, and visualize them (Graphistry).\n",
    "\n",
    "This notebook is part of the security analytics track's introductory session in RAPIDS Academy. RAPIDS Academy is a group of GPU computing leaders working together to provide tutorials and trainings for learning GPU analytics.\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "* **Video + chat**: See email/Slack: RAPIDS.ai Slack channel group + Zoom invite + GPU environment\n",
    "* **Schedule**: Introduction (Zoom), then 10min per task, with 5-10min discussion inbetween each task\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "* **Python**: Comfortable with basic Python\n",
    "* **PyData**: Helpful but not necessary: Familiarity with Jupyter Notebooks & Pandas\n",
    "* **Security**: Minimal\n",
    "\n",
    "#### Topics\n",
    "\n",
    "* **Introduction: Jupyter Python notebooks**\n",
    "  * Writing, running, & saving cells, `nvidia-smi` & GPU dashboard\n",
    "* **Task 1: Starting & loading data**\n",
    "  * `help()`, `%%time`, `cudf.read_csv()`, memory management\n",
    "  * Advanced: comparing to `pandas.read_csv` and speedup via `usecols`\n",
    "* **Task 2: Inspection & analysis** \n",
    "  * `head()`, `[[]]`, `describe()`, `count()`, `sort()`, `len()`\n",
    "  * Advanced: `unique()`, `value_counts`\n",
    "* **Task 3: Shaping & visualization**\n",
    "  * `drop_duplicates`, `groupby().agg()`, `graphistry.plot()`\n",
    "  * Advanced: `graphistry.hypergraph`\n",
    "\n",
    "### After\n",
    "* Solution notebook\n",
    "* Slack channel\n",
    "* [Subscribe to future sessions](learnrapids.com): Multi-GPU, ...\n",
    "* Guide our roadmap! RAPIDS Academy + RAPIDS ecosystem survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import RAPIDS and prepare data\n",
    "\n",
    "* For Graphistry, get a free account at [Graphistry Hub](https://www.graphistry.com/get-started) and put in graphistry.register() call below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install graphistry client library if not available \n",
    "# NOTE: No local GPU needed as it uses a remote graphistry GPU server of your choice\n",
    "# ! pip install --user graphistry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  Could not contact hub.graphistry.com. Are you connected to the Internet?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cudf': '0.14.0', 'graphistry': '0.11.2'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf, json, graphistry, pandas as pd\n",
    "from collections import OrderedDict\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "\n",
    "### Get free Graphistry Hub account & creds at https://www.graphistry.com/get-started\n",
    "### First run: set to True and fill in creds\n",
    "### Future runs: set to False and erase your creds\n",
    "### When done: delete graphistry.json\n",
    "if False:\n",
    "    #creds = {'token': '...'}\n",
    "    creds = {'username': '***', 'password': '***'}\n",
    "    with open('graphistry.json', 'w') as outfile:\n",
    "        json.dump(creds, outfile)\n",
    "with open('graphistry.json') as f:\n",
    "    creds = json.load(f)\n",
    "\n",
    "graphistry.register(\n",
    "    api=3, key='', protocol='https', server='hub.graphistry.com', \n",
    "    **creds)\n",
    "    \n",
    "\n",
    "{'cudf': cudf.__version__, 'graphistry': graphistry.__version__}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -f conn.log conn.log.gz\n",
    "! echo \"Downloading data...\"\n",
    "! wget https://www.secrepo.com/maccdc2012/conn.log.gz\n",
    "! ls -alh conn.log.gz \n",
    "! echo \"Decompressing data...\"\n",
    "! gunzip conn.log.gz\n",
    "! ls -alh conn.log\n",
    "! echo \"DONE: DOWNLOADED DATA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating sample conn10.log ...\n",
      "generating sample conn1K.log ...\n",
      "generating sample conn1M.log ...\n",
      "generating sample conn5M.log ...\n",
      "-rw-r--r-- 1 leo@graphistry.com leo@graphistry.com 2.6G Sep 21  2014 conn.log\n",
      "-rw-r--r-- 1 leo@graphistry.com leo@graphistry.com 1.4K Jul 14 19:15 conn10.log\n",
      "-rw-r--r-- 1 leo@graphistry.com leo@graphistry.com 130K Jul 14 19:15 conn1K.log\n",
      "-rw-r--r-- 1 leo@graphistry.com leo@graphistry.com 117M Jul 14 19:15 conn1M.log\n",
      "-rw-r--r-- 1 leo@graphistry.com leo@graphistry.com 575M Jul 14 19:15 conn5M.log\n",
      "DONE: SAMPLES GENERATED\n"
     ]
    }
   ],
   "source": [
    "! echo \"generating sample conn10.log ...\"\n",
    "! head -n 10 ./conn.log > conn10.log\n",
    "! echo \"generating sample conn1K.log ...\"\n",
    "! head -n 1000 ./conn.log > conn1K.log\n",
    "! echo \"generating sample conn1M.log ...\"\n",
    "! head -n 1000000 ./conn.log > conn1M.log\n",
    "! echo \"generating sample conn5M.log ...\"\n",
    "! head -n 5000000 ./conn.log > conn5M.log\n",
    "! ls -alh conn*\n",
    "! echo \"DONE: SAMPLES GENERATED\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick demo A: Jupyter\n",
    "\n",
    "1. Add cell: `+` button\n",
    "1. Run (ctrl-enter): \n",
    "```\n",
    "x = 1 + 2\n",
    "x\n",
    "```\n",
    "1. Run shell command: `ls -alh` and then `nvidia-smi`\n",
    "1. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = 1 + 1\n",
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! ls -alh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Demo B: GPU Dashboard\n",
    "\n",
    "In the left Jupyterlab menu, find the icon for the GPU dashboard extension and open the tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Load data\n",
    "\n",
    "* Read the 2.6GB file `conn.log` into a cuDF GPU dataframe using `cudf.read_csv`\n",
    "* Count the unique IPs\n",
    "* Compare the speed relative against CPU pandas\n",
    "* Intermediate + advanced: Keep GPU memory down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a: Load the data... without leaking GPU memory\n",
    "\n",
    "Load the data using `cudf.read_csv()`. It will be under the wrong format, initially.\n",
    "\n",
    "Your task is to do extra work around keeping GPU memory use under control. \n",
    "* At the end of every cell, set your dataframe variables to `None` (ex: `gdf = None`) so the garbage collector can free the memory\n",
    "* You can check GPU memory use via the dashboard and `! nvidia-smi`\n",
    "\n",
    "Start by loading sample `./conn1K.log`, and when ready, try `./conn1M.log`, `./conn5M.log`, and the full `./conn.log`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Reference\n",
    "Print the last 3 lines of `./conn1K.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1331901000.000000\\tCCUIP21wTjqkj8ZqX5\\t192.168.202.79\\t50463\\t192.168.229.251\\t80\\ttcp\\t-\\t-\\t-\\t-\\tSH\\t-\\t0\\tFa\\t1\\t52\\t1\\t52\\t(empty)\n",
      "996  1331901057.960000\\tCI1Kjf11YgCRL3jcQ4\\t192.168...                                                                                     \n",
      "997  1331901057.950000\\tCaNY2R3azdPp3cLYNg\\t192.168...                                                                                     \n",
      "998  1331901057.620000\\tCJS2P118ZwEv34NwR5\\t192.168...                                                                                     \n",
      "CPU times: user 9.37 ms, sys: 4.51 ms, total: 13.9 ms\n",
      "Wall time: 35.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log')\n",
    "\n",
    "print(gdf.tail(3))\n",
    "\n",
    "gdf=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. Task\n",
    "\n",
    "* Print the last 3 lines of `./conn5M.log` using `cudf.read_csv`\n",
    "* Use `! nvidia-smi`, the GPU monitor, and `gdf.memory_usage()` to check that memory consumption, including that it is low after setting `gdf = None`\n",
    "\n",
    "After, try on `./conn.log` and see if it can hold the 2.6GB in memory. (RAPIDS has a max str size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn5M.log')\n",
    "\n",
    "print( gdf.tail(3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "gdf = None\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b: Load 500MB\n",
    "\n",
    "Load the data using tab separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "! ls -alh conn1K.log conn5M.log conn.log\n",
    "! echo \"lines  1K: `wc -l ./conn1K.log`\"\n",
    "! echo \"lines  5M: `wc -l ./conn5M.log`\"\n",
    "! echo \"lines all: `wc -l ./conn.log`\"\n",
    "! echo\n",
    "! echo \"===========================\"\n",
    "! echo \"./conn1K.log first 3 lines:\"\n",
    "! head -n 3 ./conn1K.log\n",
    "! echo\n",
    "! echo \"===========================\"\n",
    "! echo \"via cudf.read_csv()\"\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', sep='\\t')\n",
    "print(gdf.head(3))\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. Task\n",
    "1. Read `help(cudf.read_csv)` to learn about the delimiter parameter\n",
    "1. Use the delimiter option to read the file with tab separation (`\"\\t\"`) \n",
    "1. Get it right with `./conn1K.log` and then try `./conn5M.log`\n",
    "1. ... remember to clear out your GPU memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cudf.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', #### FILL IN ###)\n",
    "\n",
    "print(gdf.head(3))\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1c: Format & load 2.6GB\n",
    "\n",
    "`read_csv` has many useful parameters to create data that is cleaner, friendlier, smaller, and faster. \n",
    "\n",
    "By using them, we will be able to load the whole dataset with out crashing, fairly quickly, and get native operations on top!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Reference\n",
    "\n",
    "* Read about parameters `names`, `dtypes`, `colnames`, and byte ranges in `cudf.read_csv()`\n",
    "* Load the Zeek format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cudf.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration',\n",
    "        'orig_bytes', 'resp_bytes', 'conn_state', \n",
    "        'local_orig?',\n",
    "        'missed_bytes', 'history', 'orig_pkts',\n",
    "        'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents']\n",
    "\n",
    "dtypes=OrderedDict([\n",
    "    ('ts', 'int64'), ('uid', 'str'),\n",
    "    ('id.orig_h', 'str'), ('id.orig_p', 'int32'), ('id.resp_h', 'str'), ('id.resp_p', 'int32'),\n",
    "    ('proto', 'str'), ('duration', 'float64'),\n",
    "    ('orig_bytes', 'int64'), ('resp_bytes', 'int64'),\n",
    "    ('conn_state', 'str'), ('local_orig?', 'str'), ('local_resp?', 'str'),\n",
    "    ('missed_bytes', 'int64'), ('history', 'str'),\n",
    "    ('orig_pkts', 'int64'), ('orig_ip_bytes', 'int64'), ('resp_pkts', 'int64'), ('resp_ip_bytes', 'int64'),\n",
    "    ('tunnel_parents', 'str')\n",
    "])\n",
    "\n",
    "# optional\n",
    "cols_subset = [\n",
    "    'ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', \n",
    "    'proto', 'duration', 'orig_bytes', 'resp_bytes'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1c. Task\n",
    "\n",
    "* Plug in `names` and `dtypes` to `read_csv`\n",
    "* ... Take care not to leak GPU memory, and try first on `./conn1K.log` before doing `./conn1M.log` / `./conn5M.log` / the full `./conn.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv(\n",
    "    ### file ###,\n",
    "    sep='\\t',\n",
    "    names=### fill in ###,\n",
    "    dtypes=### fill in ###,\n",
    "    na_values=['-', '-','(empty)'])\n",
    "\n",
    "print(gdf.dtypes)\n",
    "\n",
    "gdf.head(3)\n",
    "\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced**: Load in only the columns you will use for your analysis and compare the impact on memory & load time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Analytics & Wrangling\n",
    "\n",
    "CPU Pandas operators largely carry over to GPU cuDF. We'll take a look at activity by top IPs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a: Column manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Reference\n",
    "* df with subset of cols: `gdf[['col1', 'col2']]`\n",
    "* get one col: `gdf['col1']`\n",
    "* get df/col length: `len(gdf)` / `len(gs)`\n",
    "* series of unique elements from a series: `gs.unique()`\n",
    "* stats on one series: `gs.min()`, `gs.max()`, `gs.sum()`, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Task\n",
    "\n",
    "**Intro**: For the column of IPs `id.resp_h`, how many unique IPs are there? Start with `conn1K.log` and then try on the full `conn.log`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', \n",
    "              sep='\\t', \n",
    "              names=cols,\n",
    "              dtypes=dtypes,\n",
    "              usecols=['id.resp_h'],\n",
    "              na_values=['-', '-','(empty)'])\n",
    "\n",
    "\n",
    "unique_resp_ips = ### get 'id.resp_h' column and then its unique values ###\n",
    "\n",
    "\n",
    "print('# unique', len(unique_resp_ips))\n",
    "print(unique_resp_ips[:10])\n",
    "unique_resp_ips = None\n",
    "gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermediate/advanced**: If you have time after 2b/c, for the column of bytes `orig_bytes`, what is the biggest payload?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b: Group by & column summaries\n",
    "\n",
    "Quite powerful, you can group dataframe rows and get summary statistics for each group.\n",
    "\n",
    "In this task, we'll summarize flows between different computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Reference\n",
    "\n",
    "Pattern\n",
    "\n",
    "```python\n",
    "gdf\\\n",
    "    .groupby(['col1', 'col2', ...])\\\n",
    "    .agg({\n",
    "        'col2': ['min', 'max'],\n",
    "        'col3': 'min',\n",
    "        'col4': ['count', 'mean', 'nunique'],\n",
    "    })\\\n",
    "    .reset_index()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Task\n",
    "\n",
    "Compute summaries for `./conn1K.log` then the full `./conn.log` for:\n",
    "\n",
    "* `duration`: min/max/mean\n",
    "* `orig_bytes`: min/max/mean/sum\n",
    "* `resp_bytes`: min/max/mean/sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "gdf = cudf.read_csv('./conn1K.log', \n",
    "              sep='\\t', \n",
    "              names=cols,\n",
    "              dtypes=dtypes,\n",
    "              usecols=cols_subset,\n",
    "              na_values=['-', '-','(empty)'])\n",
    "\n",
    "\n",
    "out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "    .agg({\n",
    "        'ts': ['count', 'min', 'max', 'mean'],\n",
    "        'uid': 'nunique',\n",
    "        'id.resp_p': ['min', 'max', 'nunique'],\n",
    "        'proto': ['nunique'],\n",
    "        'duration': ### min/max/mean ###\n",
    "        'orig_bytes': ### min/max/mean/sum ###\n",
    "        'resp_bytes': ### min/max/mean/sum ###\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "gdf = None\n",
    "print(out.shape)\n",
    "print(out.dtypes)\n",
    "print(out.head(3))\n",
    "out = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advanced**: Which are the biggest exfils and longest sessions for SSH connections? Hint: use `gdf.sort_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Visualize!\n",
    "\n",
    "Graphistry lets you plot many points, and even more interesting, relationships, and visually filter + cluster them on-the-fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3a: Run Graphistry\n",
    "\n",
    "Already coded:\n",
    "\n",
    "* Nodes: IPs\n",
    "* Edges: Summarized flows\n",
    "\n",
    "**Try to -**\n",
    "\n",
    "* Pan/zoom: Similar to Google Maps - click/drag/scroll (or pinch); recenter\n",
    "* Inspect: click on a node/edge, open table inspector\n",
    "* Advanced: histogram, filter, cluster, color\n",
    "  * Add a histogram for `resp_bytes_sum` and hover over bars to see the heaviest download links\n",
    "  * Open the timebar and shift-click to create a filter for edges on the second day\n",
    "* Advanced: rendering + clustering settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows 58\n",
      "dtypes id.resp_h                    object\n",
      "id.orig_h                    object\n",
      "ts_count                      int32\n",
      "ts_min               datetime64[ns]\n",
      "ts_max               datetime64[ns]\n",
      "ts_mean              datetime64[ns]\n",
      "uid_nunique                   int32\n",
      "id.resp_p_min                 int64\n",
      "id.resp_p_max                 int64\n",
      "id.resp_p_nunique             int32\n",
      "proto_nunique                 int32\n",
      "duration_min                float64\n",
      "duration_max                float64\n",
      "duration_mean               float64\n",
      "duration_sum                float64\n",
      "orig_bytes_min              float64\n",
      "orig_bytes_max              float64\n",
      "orig_bytes_mean             float64\n",
      "orig_bytes_sum              float64\n",
      "resp_bytes_min              float64\n",
      "resp_bytes_max              float64\n",
      "resp_bytes_mean             float64\n",
      "resp_bytes_sum              float64\n",
      "dtype: object\n",
      "      id.resp_h       id.orig_h  ts_count                        ts_min  \\\n",
      "0    10.21.6.40  192.168.202.89         2 2012-03-16 12:30:32.179999744   \n",
      "1    10.61.9.10  192.168.202.89         2 2012-03-16 12:30:08.620000000   \n",
      "2  10.7.136.159  192.168.202.89         1 2012-03-16 12:30:07.089999872   \n",
      "\n",
      "                         ts_max                       ts_mean  uid_nunique  \\\n",
      "0 2012-03-16 12:30:41.189999872 2012-03-16 12:30:36.684999936            2   \n",
      "1 2012-03-16 12:30:17.629999872 2012-03-16 12:30:13.124999936            2   \n",
      "2 2012-03-16 12:30:07.089999872 2012-03-16 12:30:07.089999872            1   \n",
      "\n",
      "   id.resp_p_min  id.resp_p_max  id.resp_p_nunique  proto_nunique  \\\n",
      "0             90             90                  1              1   \n",
      "1           9091           9091                  1              1   \n",
      "2            137            137                  1              1   \n",
      "\n",
      "   duration_min  duration_max  duration_mean  duration_sum  orig_bytes_min  \\\n",
      "0          3.00          3.00           3.00          3.00             0.0   \n",
      "1          3.01          3.01           3.01          3.01             0.0   \n",
      "2          3.00          3.00           3.00          3.00           150.0   \n",
      "\n",
      "   orig_bytes_max  orig_bytes_mean  orig_bytes_sum  resp_bytes_min  \\\n",
      "0             0.0              0.0             0.0             0.0   \n",
      "1             0.0              0.0             0.0             0.0   \n",
      "2           150.0            150.0           150.0             0.0   \n",
      "\n",
      "   resp_bytes_max  resp_bytes_mean  resp_bytes_sum  \n",
      "0             0.0              0.0             0.0  \n",
      "1             0.0              0.0             0.0  \n",
      "2             0.0              0.0             0.0  \n"
     ]
    }
   ],
   "source": [
    "def compute_groupby(file='./conn.log'):\n",
    "\n",
    "    gdf = cudf.read_csv(file, \n",
    "                  sep='\\t', \n",
    "                  names=cols,\n",
    "                  dtypes=dtypes,\n",
    "                  usecols=cols_subset,\n",
    "                  na_values=['-', '-','(empty)'])\n",
    "\n",
    "    out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "        .agg({\n",
    "            'ts': ['count', 'min', 'max', 'mean'],\n",
    "            'uid': 'nunique',\n",
    "            'id.resp_p': ['min', 'max', 'nunique'],\n",
    "            'proto': ['nunique'],\n",
    "            'duration': ['min', 'max', 'mean', 'sum'],\n",
    "            'orig_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "            'resp_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "        }).reset_index()\n",
    "\n",
    "\n",
    "    ########### Data cleaning: normal column names and times as actual timestamps\n",
    "\n",
    "    out.columns = out.columns.to_flat_index() # -> col_name = (col, stat)\n",
    "    out.columns = [ '%s_%s' % c for c in out.columns ]\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        'id.resp_h_': 'id.resp_h',\n",
    "        'id.orig_h_': 'id.orig_h',\n",
    "    })\n",
    "\n",
    "    out['ts_min'] = cudf.Series(pd.to_datetime((out['ts_min']*1000000000).to_pandas()))\n",
    "    out['ts_max'] = cudf.Series(pd.to_datetime((out['ts_max']*1000000000).to_pandas()))\n",
    "    out['ts_mean'] = cudf.Series(pd.to_datetime((out['ts_mean']*1000000000).to_pandas()))\n",
    "    \n",
    "    return out\n",
    "\n",
    "out = compute_groupby('./conn1K.log')\n",
    "    \n",
    "print('# rows', len(out))\n",
    "print('dtypes', out.dtypes)\n",
    "print(out.head(3))\n",
    "\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed network, now creating plot...\n",
      "CPU times: user 5.64 s, sys: 322 ms, total: 5.97 s\n",
      "Wall time: 6.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://hub.graphistry.com/graph/graph.html?dataset=d14c52196b6443b6a5d53dadd438652e&type=arrow&viztoken=d39feb19-4345-4834-ab66-7a8820af1cfb&usertag=82077d8e-pygraphistry-0.11.2&splashAfter=1594787644&info=true'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gdf = compute_groupby('./conn.log')\n",
    "\n",
    "g = graphistry.edges(gdf).bind(source='id.orig_h', destination='id.resp_h')\n",
    "\n",
    "print('Computed network, now creating plot...')\n",
    "gdf = None\n",
    "\n",
    "#### if an error, run g.plot(render=False)\n",
    "g.plot(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3b: Map SSH activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Copy & rename `compute_groupby` as `compute_groupby2` \n",
    "* Filter connections to just SSH traffic (port 22) based on column `id.resp_p` \n",
    "* Plot\n",
    "* Answer: Which link transferred the most data out over SSH, and is that typical of the sender/receiver?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Reference\n",
    "\n",
    "Hint: To filter, `gdf2 = gdf[ gdf['some_col'] == some_val]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b. Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_groupby2(file='./conn1K.log'):\n",
    "\n",
    "    gdf = cudf.read_csv(file, \n",
    "                  sep='\\t', \n",
    "                  names=cols,\n",
    "                  dtypes=dtypes,\n",
    "                  usecols=cols_subset,\n",
    "                  na_values=['-', '-','(empty)'])\n",
    "    \n",
    "    gdf = gdf[ gdf['id.resp_p'] == 22 ]\n",
    "\n",
    "    out = gdf.groupby(['id.resp_h', 'id.orig_h'])\\\n",
    "        .agg({\n",
    "            'ts': ['count', 'min', 'max', 'mean'],\n",
    "            'uid': 'nunique',\n",
    "            'id.resp_p': ['min', 'max', 'nunique'],\n",
    "            'proto': ['nunique'],\n",
    "            'duration': ['min', 'max', 'mean', 'sum'],\n",
    "            'orig_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "            'resp_bytes': ['min', 'max', 'mean', 'sum'],\n",
    "        }).reset_index()\n",
    "\n",
    "\n",
    "    ########### Data cleaning: normal column names and times as actual timestamps\n",
    "\n",
    "    out.columns = out.columns.to_flat_index() # -> col_name = (col, stat)\n",
    "    out.columns = [ '%s_%s' % c for c in out.columns ]\n",
    "\n",
    "    out = out.rename(columns={\n",
    "        'id.resp_h_': 'id.resp_h',\n",
    "        'id.orig_h_': 'id.orig_h',\n",
    "    })\n",
    "\n",
    "    out['ts_min'] = cudf.Series(pd.to_datetime((out['ts_min']*1000000000).to_pandas()))\n",
    "    out['ts_max'] = cudf.Series(pd.to_datetime((out['ts_max']*1000000000).to_pandas()))\n",
    "    out['ts_mean'] = cudf.Series(pd.to_datetime((out['ts_mean']*1000000000).to_pandas()))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows 4\n",
      "dtypes id.resp_h                    object\n",
      "id.orig_h                    object\n",
      "ts_count                      int32\n",
      "ts_min               datetime64[ns]\n",
      "ts_max               datetime64[ns]\n",
      "ts_mean              datetime64[ns]\n",
      "uid_nunique                   int32\n",
      "id.resp_p_min                 int64\n",
      "id.resp_p_max                 int64\n",
      "id.resp_p_nunique             int32\n",
      "proto_nunique                 int32\n",
      "duration_min                float64\n",
      "duration_max                float64\n",
      "duration_mean               float64\n",
      "duration_sum                float64\n",
      "orig_bytes_min              float64\n",
      "orig_bytes_max              float64\n",
      "orig_bytes_mean             float64\n",
      "orig_bytes_sum              float64\n",
      "resp_bytes_min              float64\n",
      "resp_bytes_max              float64\n",
      "resp_bytes_mean             float64\n",
      "resp_bytes_sum              float64\n",
      "dtype: object\n",
      "        id.resp_h       id.orig_h  ts_count                        ts_min  \\\n",
      "0  192.168.23.254  192.168.202.68         1 2012-03-16 12:30:30.210000128   \n",
      "1  192.168.26.254  192.168.202.68         1 2012-03-16 12:30:32.029999872   \n",
      "2  192.168.27.102  192.168.202.68         1 2012-03-16 12:30:34.329999872   \n",
      "\n",
      "                         ts_max                       ts_mean  uid_nunique  \\\n",
      "0 2012-03-16 12:30:30.210000128 2012-03-16 12:30:30.210000128            1   \n",
      "1 2012-03-16 12:30:32.029999872 2012-03-16 12:30:32.029999872            1   \n",
      "2 2012-03-16 12:30:34.329999872 2012-03-16 12:30:34.329999872            1   \n",
      "\n",
      "   id.resp_p_min  id.resp_p_max  id.resp_p_nunique  proto_nunique  \\\n",
      "0             22             22                  1              1   \n",
      "1             22             22                  1              1   \n",
      "2             22             22                  1              1   \n",
      "\n",
      "   duration_min  duration_max  duration_mean  duration_sum  orig_bytes_min  \\\n",
      "0          1.81          1.81           1.81          1.81           885.0   \n",
      "1          2.29          2.29           2.29          2.29           885.0   \n",
      "2          7.56          7.56           7.56          7.56           909.0   \n",
      "\n",
      "   orig_bytes_max  orig_bytes_mean  orig_bytes_sum  resp_bytes_min  \\\n",
      "0           885.0            885.0           885.0           868.0   \n",
      "1           885.0            885.0           885.0           868.0   \n",
      "2           909.0            909.0           909.0          1887.0   \n",
      "\n",
      "   resp_bytes_max  resp_bytes_mean  resp_bytes_sum  \n",
      "0           868.0            868.0           868.0  \n",
      "1           868.0            868.0           868.0  \n",
      "2          1887.0           1887.0          1887.0  \n"
     ]
    }
   ],
   "source": [
    "### Test\n",
    "\n",
    "out = compute_groupby2('./conn1K.log')\n",
    "    \n",
    "print('# rows', len(out))\n",
    "print('dtypes', out.dtypes)\n",
    "print(out.head(3))\n",
    "\n",
    "out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.52 s, sys: 237 ms, total: 1.75 s\n",
      "Wall time: 2.16 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://hub.graphistry.com/graph/graph.html?dataset=d5f6d907ad784bdf8d2bac6d91f87be8&type=arrow&viztoken=cad9999f-3b37-473f-a1bd-e3b2ac592096&usertag=82077d8e-pygraphistry-0.11.2&splashAfter=1594788176&info=true'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Plot\n",
    "\n",
    "%%time\n",
    "\n",
    "gdf = compute_groupby2('./conn.log')\n",
    "\n",
    "g = graphistry.edges(gdf).bind(source='id.orig_h', destination='id.resp_h')\n",
    "\n",
    "gdf = None\n",
    "\n",
    "####run g.plot(render=False) if cannot render inline\n",
    "g.plot(render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After\n",
    "\n",
    "#### Do\n",
    "* Solution notebook\n",
    "* Slack channel\n",
    "* [Subscribe to future sessions](learnrapids.com): Multi-GPU, ...\n",
    "* Guide our roadmap! RAPIDS Academy + RAPIDS ecosystem survey\n",
    "\n",
    "#### References\n",
    "* [10 minutes to pandas](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html)\n",
    "* 2.6GB of Zeek connection logs (22M rows) from https://www.secrepo.com/ \n",
    "* RAPIDS docs: https://docs.rapids.ai/api/cudf/stable/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "rapids-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
