{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The DataFrame\n",
    "\n",
    "The RAPIDS ecosystem is built on the concept of a `cudf.DataFrame`, built on [Apache Arrow](https://github.com/apache/arrow), shared between all of the different libraries and packages.\n",
    "\n",
    "There are two libraries specific to data manipulation:\n",
    "- BlazingSQL: SQL commands on a `cudf.DataFrame`\n",
    "- cuDF: pandas-like commands on a `cudf.DataFrame`\n",
    "\n",
    "## BlazingSQL (BSQL)\n",
    "[GitHub](https://github.com/BlazingDB/blazingsql)\n",
    "\n",
    "As mentioned in the [Welcome Notebook](../welcome.ipynb), BlazingSQL is a SQL engine built on top of cuDF, the `cudf.DataFrame` package. This means you can easily chain together SQL queries in Python and the RAPIDS ecosystem to build complex and scalable data pipelines for machine learning, graph analytics, and more.\n",
    "\n",
    "We'll show off a series of examples that demonstrate the power of BlazingSQL.\n",
    "\n",
    "#### SQL Query a `cudf.DataFrame`\n",
    "To start we always need to make a BlazingContext. The BlazingContext is a live session with the SQL engine, and stores information such as created tables, registered storage plugins, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazingsql import BlazingContext\n",
    "\n",
    "bc = BlazingContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can create a cuDF as if it were a pandas DataFrame, and make a BSQL table off of it using the `.create_table()` method. This is a zero-copy process, meaning it is very fast, and won't take up more space on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "\n",
    "# read CSV file into cuDF DataFrame\n",
    "df = cudf.read_csv('../data/sample_taxi.csv')\n",
    "\n",
    "# create table from cuDF DataFrame\n",
    "bc.create_table('taxi', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no run a SQL query on that `cudf.DataFrame` with the `.sql()` method which returns a new cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.sql('''\n",
    "       select \n",
    "           cast(substring(tpep_pickup_datetime,0,10) || ' 00:00:00' as timestamp) as pickup_date, \n",
    "           count(*), \n",
    "           avg(trip_distance), \n",
    "           avg(fare_amount) \n",
    "       from taxi \n",
    "           group by \n",
    "               cast(substring(tpep_pickup_datetime,0,10) || ' 00:00:00' as timestamp)\n",
    "           order by \n",
    "               cast(substring(tpep_pickup_datetime,0,10) || ' 00:00:00' as timestamp)\n",
    "           limit 10\n",
    "           ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Query a CSV File\n",
    "We could also save ourselves some time and run `.create_table()` directly on a [supported file format](https://docs.blazingdb.com/docs/text-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('taxi', '../data/sample_taxi.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bc.sql('select count(*) from taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL Query a Data Lake (AWS S3)\n",
    "You can register multiple [Storage Plugins](https://docs.blazingdb.com/docs/connecting-data-sources) on a BlazingContext. These Storage Plugins help BSQL optimize IO requests during query execution. For example, with [Apache Parquet](https://parquet.apache.org/) files on AWS S3, BSQL can register AWS S3 buckets with the `.s3('name', bucket_name='bucket_name')` method, and will read the metadata and skip files/partitions based on a SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.s3('blazingsql-colab', bucket_name='blazingsql-colab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can reference files in AWS S3 with a filesystem path convention `s3://storage_plugin_name/path_to_file...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.create_table('big_taxi', 's3://blazingsql-colab/yellow_taxi/1_0_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bc.sql('select count(*) from big_taxi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the most common features leveraged in BSQL. What makes it such a powerful and extensible tool though is the fact that everything in-GPU-memory is a `cudf.DataFrame`.\n",
    "\n",
    "Let's now learn about cuDF.\n",
    "\n",
    "## cuDF\n",
    "[GitHub](https://github.com/rapidsai/cudf)\n",
    "\n",
    "As mentioned in the [Welcome Notebook](../welcome.ipynb), cuDF is a pandas-like DataFrame library. cuDF is almost a drop-in replacement for pandas, the main difference is that it is operating on GPU memory. We'll walk through some examples that demonstrate how to use cuDF.\n",
    "\n",
    "Parts of this were borrowed and lightly adapted from [10 Minutes to cuDF and Dask-cudf](https://rapidsai.github.io/projects/cudf/en/0.12.0/10min.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a simple `cudf.Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = cudf.Series([1, 2, 3, None, 4])\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `cudf.DataFrame` with 1000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "df = cudf.DataFrame(\n",
    "                    {'a':range(n),\n",
    "                     'b':range(500, n + 500),\n",
    "                     'c':range(1000, n + 1000)}\n",
    "                   )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `cudf.DataFrame` can be treated like a `pandas.DataFrame`.\n",
    "\n",
    "Such as sorting by values in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='b', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection by position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:3, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or performing a `Join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = cudf.DataFrame()\n",
    "df_a['key'] = ['a', 'b', 'c', 'd', 'e']\n",
    "df_a['vals_a'] = [float(i + 10) for i in range(5)]\n",
    "\n",
    "df_b = cudf.DataFrame()\n",
    "df_b['key'] = ['a', 'c', 'e']\n",
    "df_b['vals_b'] = [float(i+100) for i in range(3)]\n",
    "\n",
    "merged = df_a.merge(df_b, on=['key'], how='left')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use BSQL and cuDF together to chain together a data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bc.sql('select * from taxi where trip_distance < 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the `.describe()` method to better understand taxi rides under 10 miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quicky convert to `pandas.DataFrame` and immediately integrate with anything that supports pandas.\n",
    "\n",
    "Below is an example with [Matplotlib](https://github.com/matplotlib/matplotlib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pandas().plot(kind='scatter', x='passenger_count', y='tip_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask cuDF \n",
    "\n",
    "[Docs](https://docs.rapids.ai/api/cudf/stable/dask-cudf.html)\n",
    "\n",
    "cuDF is a single-GPU library. For Multi-GPU cuDF solutions we use Dask and the dask-cudf package , which is able to scale cuDF across multiple GPUs on a single machine, or multiple GPUs across many machines in a cluster.\n",
    "\n",
    "Dask DataFrame was originally designed to scale Pandas, orchestrating many Pandas DataFrames spread across many CPUs into a cohesive parallel DataFrame. Because cuDF currently implements only a subset of Pandasâ€™s API, not all Dask DataFrame operations work with cuDF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_cudf\n",
    "\n",
    "df = dask_cudf.read_parquet(\"../data/blobs.parquet\")\n",
    "\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `.compute()`, `.head()` or `.tail()` on a `dask_cudf.DataFrame` returns a `cudf.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlazingSQL Distributed\n",
    "\n",
    "[Docs](https://docs.blazingdb.com/docs/distributed)\n",
    "\n",
    "BlazingSQL can easily distribute query execution across multiple GPUs or servers with Dask. You don't have to pass a list of IPs and ports to BSQL, whatever you configure with Dask will give your BlazingContext instance awareness of where all the GPUs or servers are. Check out blog_posts/[distributed_sql_with_dask.ipynb](../blog_posts/distributed_sql_with_dask.ipynb) or [Distributed SQL with Dask](https://blog.blazingdb.com/distributed-sql-with-dask-2979262acc8a?source=friends_link&sk=077319064cd7d9e18df8c0292eb5d33d) for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazingsql import BlazingContext\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "bc = BlazingContext(dask_client=client, network_interface='lo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a public AWS S3 bucket, then create & query a table (`big_taxi`) from there.\n",
    "\n",
    "When BlazingSQL runs on multiple GPUs, query results will return as `dask_cudf.DataFrame`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.s3('blazingsql-colab', bucket_name='blazingsql-colab')\n",
    "\n",
    "bc.create_table('big_taxi', 's3://blazingsql-colab/yellow_taxi/1_0_0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bc.sql('select count(*) from big_taxi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc.sql('select * from big_taxi where trip_distance < 10 limit 5').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That is the DataFrame Tour!\n",
    "You've seen the basics of the DataFrame and how you interact at with it. Now is a good time to experiment with your own data and see how to parse, clean, and extract meaningful insights from it.\n",
    "\n",
    "We'll also get into how to run visualization either with popular Python visualization packages, as well as GPU-accelerated visualization packages.\n",
    "\n",
    "\n",
    "[Continue to the Data Visualization introductory Notebook](data_visualization.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAPIDS Stable",
   "language": "python",
   "name": "rapids-stable"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
